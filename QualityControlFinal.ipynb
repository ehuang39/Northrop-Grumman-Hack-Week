{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Team 12\n",
        "\n",
        "Members: Ankan Roy, Anthony Cheng, Eric Huang, and Viraj Boreda\n",
        "\n",
        "Mentor: Daphney Chery"
      ],
      "metadata": {
        "id": "PlvJcuCD0cac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step is to install and import the necessary Python libraries, and add the Google Drive folder to Colab."
      ],
      "metadata": {
        "id": "XZGfnoYT2EDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install any necessary libraries"
      ],
      "metadata": {
        "id": "vDSLcUI90dUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from google.colab import drive\n",
        "from google.colab.patches import cv2_imshow\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import joblib"
      ],
      "metadata": {
        "id": "4QwewGhu0fMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "id": "4M6YX5P-vphn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we add the files to Google Drive so we can access them in Colab."
      ],
      "metadata": {
        "id": "StcPbwwB28ZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "defective_dir = '/gdrive/MyDrive/InnovationHackWeek/TRAIN_def_front'\n",
        "ok_dir = '/gdrive/MyDrive/InnovationHackWeek/TRAIN_ok_front'\n",
        "defective_files = os.listdir(defective_dir)\n",
        "ok_files = os.listdir(ok_dir)"
      ],
      "metadata": {
        "id": "BQL-hMYV4KPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we use OpenCV to write a function to remove shadows from the images."
      ],
      "metadata": {
        "id": "svKSfSYs5xFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_shadow(img):\n",
        "    rgb_planes = cv2.split(img)\n",
        "    result_planes = []\n",
        "    result_norm_planes = []\n",
        "    for plane in rgb_planes:\n",
        "        dilated_img = cv2.dilate(plane, np.ones((7,7), np.uint8))\n",
        "        bg_img = cv2.medianBlur(dilated_img, 21)\n",
        "        diff_img = 255 - cv2.absdiff(plane, bg_img)\n",
        "        norm_img = cv2.normalize(diff_img,None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8UC1)\n",
        "        result_planes.append(diff_img)\n",
        "        result_norm_planes.append(norm_img)\n",
        "\n",
        "    result = cv2.merge(result_planes)\n",
        "    result_norm = cv2.merge(result_norm_planes)\n",
        "    return result, result_norm"
      ],
      "metadata": {
        "id": "V_DAiCFv5x3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After that, we use OpenCV to clean up the defective & ok images. We got rid of shadows, made the images grayscale, then thresholded the images. Finally, we detected contours and wrote their areas into a list."
      ],
      "metadata": {
        "id": "MvsuXcLr2XzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "defective_contour_list = []\n",
        "\n",
        "for filename in defective_files:\n",
        "  img = cv2.imread(defective_dir + \"/\" + filename)\n",
        "\n",
        "  # Get rid of shadows\n",
        "  result1, result2 = remove_shadow(img)\n",
        "\n",
        "  # Detect the contours on the binary image using cv2.CHAIN_APPROX_NONE\n",
        "  img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "  ret, thresh = cv2.threshold(img_gray, 50, 255, cv2.THRESH_BINARY)\n",
        "  contours, hierarchy = cv2.findContours(image=thresh, mode=cv2.RETR_TREE, method=cv2.CHAIN_APPROX_NONE)\n",
        "\n",
        "  # Calculate the total area and count the number of contours\n",
        "  total_area = 0\n",
        "  num_contours = len(contours)\n",
        "  contour_lst2 = []\n",
        "  for contour in contours:\n",
        "      contour_lst2.append(cv2.contourArea(contour))\n",
        "\n",
        "  defective_contour_list.append(contour_lst2)"
      ],
      "metadata": {
        "id": "-xm5ptbr6JzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ok_contour_list = []\n",
        "\n",
        "for filename in ok_files:\n",
        "  img = cv2.imread(ok_dir + \"/\" + filename)\n",
        "\n",
        "  # Get rid of shadows\n",
        "  result1, result2 = remove_shadow(img)\n",
        "\n",
        "  # Detect the contours on the binary image using cv2.CHAIN_APPROX_NONE\n",
        "  img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "  ret, thresh = cv2.threshold(img_gray, 50, 255, cv2.THRESH_BINARY)\n",
        "  contours, hierarchy = cv2.findContours(image=thresh, mode=cv2.RETR_TREE, method=cv2.CHAIN_APPROX_NONE)\n",
        "\n",
        "  # Calculate the total area and count the number of contours\n",
        "  total_area = 0\n",
        "  num_contours = len(contours)\n",
        "  contour_lst = []\n",
        "  for contour in contours:\n",
        "      contour_lst.append(cv2.contourArea(contour))\n",
        "\n",
        "  ok_contour_list.append(contour_lst)"
      ],
      "metadata": {
        "id": "aTO3MCO37yHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we created the training x and y inputs. We combined the defective and ok image contour area lists into x train, and created a y train list that is 1 for all defective images and 0 for all ok images. 0 and 1 are our output labels for ok and defective respectively.\n",
        "\n",
        "We only take the first 265 contour areas as features for the ML classifier, as additional ones are not necessary."
      ],
      "metadata": {
        "id": "yOgPjeNe7_UP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_temp = defective_contour_list + ok_contour_list\n",
        "y_train = [1 for x in range(706)] + [0 for x in range(506)]\n",
        "\n",
        "x_train = []\n",
        "for xt in x_train_temp:\n",
        "    xt_new = xt + [0] * (265 - len(xt))\n",
        "    xt_new = xt_new[:265]\n",
        "    x_train.append(xt_new)"
      ],
      "metadata": {
        "id": "CZNPZvEo7_sI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we create our Random Forest Classifier using scikit-learn and fit it on the x-train and y-train data. We have displayed the training accuracy below."
      ],
      "metadata": {
        "id": "lwk6ISAc80QY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = RandomForestClassifier(n_estimators=50,max_depth=8)\n",
        "model.fit(x_train,y_train)\n",
        "accuracy = model.score(x_train, y_train)\n",
        "print(accuracy)"
      ],
      "metadata": {
        "id": "BgWTYfMj8zyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below saves the model to a file."
      ],
      "metadata": {
        "id": "C7kFJQzPAxIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Will not run now because we don't want to overwrite our existing model\n",
        "joblib.dump(model, '/gdrive/MyDrive/InnovationHackWeek/quality_control_model.joblib')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etiTxBoFAzw7",
        "outputId": "8c3842d5-8566-47ff-f961-a548da4dedd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/gdrive/MyDrive/InnovationHackWeek/quality_control_model.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below loads the model from joblib, as well as the testing data from Google Drive."
      ],
      "metadata": {
        "id": "kZ9yqWYe2l1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = joblib.load('/gdrive/MyDrive/InnovationHackWeek/quality_control_model.joblib')\n",
        "\n",
        "test_dir_ok = '/gdrive/MyDrive/InnovationHackWeek/TEST_ok_front'\n",
        "test_dir_def = '/gdrive/MyDrive/InnovationHackWeek/TEST_def_front'\n",
        "\n",
        "ok_files = os.listdir(test_dir_ok)\n",
        "def_files = os.listdir(test_dir_def)"
      ],
      "metadata": {
        "id": "El2r8AN2j_oB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code uses OpenCV to turn the test images into arrays of numbers so they can be used to make predictions."
      ],
      "metadata": {
        "id": "fwlYtXr0nYoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "defective_contour_list = []\n",
        "\n",
        "for filename in def_files:\n",
        "  img = cv2.imread(test_dir_def + \"/\" + filename)\n",
        "\n",
        "  # Get rid of shadows\n",
        "  result1, result2 = remove_shadow(img)\n",
        "\n",
        "  # Detect the contours on the binary image using cv2.CHAIN_APPROX_NONE\n",
        "  img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "  ret, thresh = cv2.threshold(img_gray, 50, 255, cv2.THRESH_BINARY)\n",
        "  contours, hierarchy = cv2.findContours(image=thresh, mode=cv2.RETR_TREE, method=cv2.CHAIN_APPROX_NONE)\n",
        "\n",
        "  # Calculate the total area and count the number of contours\n",
        "  total_area = 0\n",
        "  num_contours = len(contours)\n",
        "  contour_lst2 = []\n",
        "  for contour in contours:\n",
        "      contour_lst2.append(cv2.contourArea(contour))\n",
        "\n",
        "  defective_contour_list.append(contour_lst2)"
      ],
      "metadata": {
        "id": "XuBEUm4ii0OV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ok_contour_list = []\n",
        "\n",
        "for filename in ok_files:\n",
        "  img = cv2.imread(test_dir_ok + \"/\" + filename)\n",
        "\n",
        "  # Get rid of shadows\n",
        "  result1, result2 = remove_shadow(img)\n",
        "\n",
        "  # Detect the contours on the binary image using cv2.CHAIN_APPROX_NONE\n",
        "  img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "  ret, thresh = cv2.threshold(img_gray, 50, 255, cv2.THRESH_BINARY)\n",
        "  contours, hierarchy = cv2.findContours(image=thresh, mode=cv2.RETR_TREE, method=cv2.CHAIN_APPROX_NONE)\n",
        "\n",
        "  # Calculate the total area and count the number of contours\n",
        "  total_area = 0\n",
        "  num_contours = len(contours)\n",
        "  contour_lst = []\n",
        "  for contour in contours:\n",
        "      contour_lst.append(cv2.contourArea(contour))\n",
        "\n",
        "  ok_contour_list.append(contour_lst)"
      ],
      "metadata": {
        "id": "3oYGqn6pi2M_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test_temp = defective_contour_list + ok_contour_list\n",
        "y_test = [1 for x in range(len(defective_contour_list))] + [0 for x in range(len(ok_contour_list))]\n",
        "\n",
        "x_test = []\n",
        "for xt in x_test_temp:\n",
        "    xt_new = xt + [0] * (265 - len(xt))\n",
        "    xt_new = xt_new[:265]\n",
        "    x_test.append(xt_new)"
      ],
      "metadata": {
        "id": "TxEIhrzri3k_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we make predictions using the loaded RandomForestClassifier, and output the prediction accuracy."
      ],
      "metadata": {
        "id": "rU-lah0inl-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  predictions = model.predict(x_test)\n",
        "\n",
        "correct_count = 0\n",
        "for i in range(len(predictions)):\n",
        "  if predictions[i] == y_test[i]:\n",
        "    correct_count += 1\n",
        "print(\"Number of predictions: \" + str(len(y_test)))\n",
        "print(\"Number of correct predictions: \" + str(correct_count))\n",
        "print(\"Percentage of correct predictions: \" + str(correct_count / len(y_test)))\n",
        "\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "4ys6_7Wdi5IM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c9c61a2-ebb8-4982-fb6c-d75ba5560620"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of predictions: 88\n",
            "Number of correct predictions: 70\n",
            "Percentage of correct predictions: 0.7954545454545454\n",
            "[1 1 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
            " 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
            " 1 1 0 1 1 0 1 1 0 1 0 0 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are some additional statistics about the test data."
      ],
      "metadata": {
        "id": "rbIXEO3Mnrzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "defective_count = sum(predictions)\n",
        "non_defective_count = len(predictions) - defective_count\n",
        "\n",
        "print(f\"Defective items in the test data: {defective_count}\")\n",
        "print(f\"Non-defective items in the test data: {non_defective_count}\")"
      ],
      "metadata": {
        "id": "VZ9t55gIi6ZQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23a153d9-c548-4152-ad51-483565d0316c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defective items in the test data: 71\n",
            "Non-defective items in the test data: 17\n"
          ]
        }
      ]
    }
  ]
}